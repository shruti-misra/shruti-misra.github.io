<!DOCTYPE html>
<html lang="en">
   <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Portfolio" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&display=swap"
      rel="stylesheet"
    />
    <script
      src="https://kit.fontawesome.com/fa62c117c7.js"
      crossorigin="anonymous"
    ></script>
    <link rel="stylesheet" href="styles.css" />
    <title>Shruti Misra</title>
  </head>
<body class="light" id="top">
       <header class="header center">
      <h3>
        <a href="https://shruti-misra.github.io/#top" class="link"
          >SM.</a
        >
      </h3>

      <nav class="nav center">
         <ul class="nav__list center">
          <li class="nav__list-item">
            <a class="link link--nav" href="https://shruti-misra.github.io/#projects">Projects</a>
          </li>
          <li class="nav__list-item">
            <a class="link link--nav" href="https://shruti-misra.github.io/#skills">Skills</a>
          </li>
          <li class="nav__list-item">
            <a class="link link--nav" href="https://shruti-misra.github.io/#contact">Contact</a>
          </li>
        </ul>

        <button type="button" aria-label="toggle theme" class="btn btn--icon">
          <i aria-hidden="true" id="btn-theme" class="fas fa-moon"></i>
        </button>

        <button
          type="button"
          aria-label="toggle navigation"
          class="btn btn--icon nav__hamburger"
        >
          <i aria-hidden="true" class="fas fa-bars"></i>
        </button>
      </nav>
    </header>
      
<section class="section problem" id="problem">
<h2 class="section__title">Problem</h2>
<p class="content">
Microsoft's cloud-based platform for real-time AI serving, Brainwave, uses high-performance field-programmable gate arrays (FPGAs) to accelerate inferencing. Project Brainwave inferencing for services such as Bing and Azure. For this internship, my team, which served Bing wanted to know whether inferencing could be further accelerated by supporting sparse computations in Brainwave's BERT framework. To address this, I performed the following tasks:

<ul class="content" style="list-style-type:disc; padding-left: 20px;">
<li>Analyzed the feasibility of supporting sparse computations within the Self Attention function of Brainwave's BERT framework.</li>
<li>Designed, implement/modify, and test firmware to support sparse computations in BERT, ensuring seamless integration with the existing framework.</li>
<li>Identified and suggest ways to extend the architecture to better support sparsity, thus improving the overall performance of the framework.</li>
</ul>   
<p class="content">Through these tasks, I demonstrated the speed gains and latency reductions for the existing BERT framework if sparse computations were used. However, I also identified limitations that needed to be addressed before putting sparse computations into production.</p>
</p>

</section>
  
<section class="section bert" id="bert">
<h2 class="section__title">What is BERT?</h2>
<p class="content">
BERT is a pre-trained language model designed for various general natural language processing (NLP) tasks, including but not limited to question answering, next sentence prediction, and sequence classification. This versatile model can be fine-tuned for specific tasks, making it a valuable tool for a wide range of NLP applications.

</p>
</section>

<section class="section sparse" id="sparse">
<h2 class="section__title">Sparse Transformers</h2>
<p class="content">

<img align="center" src="assets/sparse.png" width= "400" height = "300">
<p class="content">The full transformer has limitations, particularly in terms of its memory and computational requirements, which increase quadratically as the sequence length grows. Sparse transformers refer to a variant of the transformer architecture that aims to reduce the computational complexity of the original model while maintaining or improving its performance. This is achieved by focusing on the most relevant parts of the input sequence, rather than processing the entire sequence at once. By using sparsity to limit the amount of information that needs to be processed, Sparse Transformers have shown promise in achieving state-of-the-art results on several natural language processing tasks with significantly reduced computation time and resource requirements.</p>

</p>
</section>

<section class="section solution" id="solution">
<h2 class="section__title">Solution Approach</h2>
<p class="content">
Brainwave performs computation in the form of tiles consisting of matrices of input. My approach was to modify the tile engine such that it performs computation only on the tiles that have values in it.

<section class="section objective" id="objective">
<h3 class="section__title">Objectives</h3>
<ul class="content" style="list-style-type:disc; padding-left: 20px;">
<li>Develop a method for computing only the tiles comprising the lower triangular half of the attention score matrix, thus reducing computational requirements.</li>
<li>Implement this method for any given arbitrary sparse pattern, regardless of sequence length.</li>
<li>Analyze the impact of different sparse patterns and their parameters (such as sequence length and window size) on the performance of the model.</li>
<li>Explore hardware and architectural changes that could be made to the existing functionality to further leverage sparsity and accelerate attention computation.</li>
</ul>
</section>

<section class="section results" id="results">   
<h3 class="section__title">Results</h3>

<p class="content">The results of this project demonstrate:</p>
<ul class="content" style="list-style-type:disc; padding-left: 20px;">
<li>A significant decrease in latency when adding support for sparse computations to the attention mechanism. This decrease in latency is found to scale with sequence length and sparsity, indicating the potential for even greater improvements with larger and sparser datasets.</li>
<li>Some anomalous behavior and overhead within the context computation needed further investigation to ensure optimal performance.</li>
<li>Packing dense tiles together has been identified as a means of saving VRF space, providing additional benefits in terms of resource utilization.</li>
</ul>
<p class="content">Overall, these results offer promising insights into the potential of sparse computations to improve the efficiency and effectiveness of natural language processing models, paving the way for further research and development in this area.</p>
</section>

<section class="section challenges" id="challenges">      
<h3 class="section__title">Challenges</h3>

This project encountered several challenges related to supporting large sequence lengths in BERT:
<ul class="content" style="list-style-type:disc; padding-left: 20px;">
<li>Limited memory in the BERT SKU makes it currently infeasible to support large sequence lengths.</li>
<li>All testing was done in an emulator with unrealistic VRF sizes, which could impact the accuracy and performance of the model in real-world scenarios.</li>
<li>The lack of a pre-trained model limited my ability to conduct a comprehensive analysis of the impact of increased sparsity on accuracy and model performance.</li>
</ul>
<p class="content">Addressing these challenges will require further research and development, including the exploration of new hardware solutions and the creation of more comprehensive testing environments to better evaluate the impact of sparsity on model accuracy and performance.</p>
</section>

<section class="section learnings" id="learnings">  
<h3 class="section__title">Learnings</h3>
   
<p class="content">This project offered several valuable learnings:</p>
<ul class="content" style="list-style-type:disc; padding-left: 20px;">
<li>Modifying existing firmware to integrate a new model can be challenging but also rewarding and enjoyable.</li>
<li>There may be a tradeoff between the robustness offered by sparse patterns and computational performance, which needs to be carefully considered when designing and implementing sparse transformers.</li>
<li>The end-to-end process of breaking down a new model and porting it to hardware can be complex and challenging, but it provides a valuable engineering lesson on how to optimize performance and improve the efficiency of deep learning models on hardware platforms.</li>
</ul>
</p>
</section>
</section>
  </body>
</html>


